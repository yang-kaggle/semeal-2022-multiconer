是针对22年task 11 MultiCoNER比赛的记录，基本没有参考价值

## 算法（印度老哥）

#### 1 预处理

都是针对csv文件处理，text和label的表格，text就是一句话，label就是一句每个单词的token

#### 2 词嵌入

根据什么模型把text转换成向量，一个单词对应一个向量，**问题在于词嵌入后的单词数量变多**

如一段text含有5单词，对应5个label，但是产生7*50词嵌入向量，7和5的数量对应不上

同样的，label也需要转换成数字形势，采用字典、集合实现

#### 3 再处理

老哥采取的办法是原来多少单词就减少到多少

对一段text，遍历每个单词，单词会产生多的词嵌入

对于同一单词的分词向量，采用首个、末尾、平均代表该单词的向量

这样有多少词向量对应多少标签

####  5 小批量

又产生另一个问题，不同text的单词数量不同，无法小批量处理

对此就用特殊字pad填充统一text的单词长度，长了就截断

对应了label也要填充特殊字pad的label

#### 6 建模

模型就很简单

一个小批量输入就是num_sample *  num_words，embedding_dim

在loss函数中，输出的num_sample *  num_words, num_label，标准label也应该拉伸为num_sample * num_words

---



## 算法（wj）

#### 1 预处理

其实差不多，还是只不过前面一定要转换成csv，脱裤子放屁

输入还是text，不过不是一句话了，而是单词列表（一个text对应一个单词列表，对应一个label列表）

#### 2 词嵌入

还是老问题，词嵌入后长度和实际标签不符，

老哥是缩短词嵌入长度，这里是扩大标签长度：**分词后的首个单词的标签不变，其余副分词标签为-100**

老哥以label长度为标准，这里以的词嵌入长度为标准

其中distbert为了便于找对应关系，使用了词嵌入属性offset

其他模型的offset不好用，得通过词嵌入找到源单词，副分词会有特殊标记

---



## 名字关系

#### sentence\<word\>：原始文本，可以是一段话，也可也是一行一个单词（不同段的用空格分割）

#### encoding=embedding：词嵌入，通常一个单词会被拆分多个分词

- 一个enconding内包含很多属性：input_id，attention_mask，offset_mapping

#### tag：文本每个单词对应的实体名称

#### label\<id\>：每个实体名字对应的数字表示

---



### 可改进

- 模型选择 HugginFace
- 视频参考 B站
- 评测方法 —— 基本完成
- 学习率设置
- label对齐方案
- 模型保存



#### 1.14 问题

用bert多语言模型训练了第一次，

应该是epoch不够或者学习率太小

用roberta多语言效果也是不理想，f1很低，但是也可以增长的

然后就是上述都是mix，这个数据集很小的

后来发现multi很大，训练应该基于这个

multi数据集验证都要很多时间，还不能一次性放入模型，得构成数据集分批次放入，好在能一次性解码

评测方法方面基本完成，就调库嘛



#### 1.18 问题

F1上不去啊，妈的代码写错了，错过了几个可能比较好的模型

虽然在摆动，但是还是有可能上升的，多加几个epoch试试，不过我想顶多到70%

学习率也是一样的，调整好像有效果，不过不明显，在60%上不去；另，注意到如果metric有报错，说明预测结果清一色一个东西，这样没法评测的，多半是学习率太大了

label对齐方案好像差别不大，没有具体探究，默认对应首个其余-100方案；若另一种方案，可在此基础上设置解码方案，选择最多的，不过很多细节，挺麻烦实现的；

试试加入dropout层，就要重写代码，就挺麻烦的

然后再看看换个模型啥的；对还有B站老哥视频可以参考
